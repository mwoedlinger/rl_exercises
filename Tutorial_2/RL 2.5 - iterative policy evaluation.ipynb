{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cf03a1f",
   "metadata": {},
   "source": [
    "### Iterative policy evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d2653a",
   "metadata": {},
   "source": [
    "Implement iterative policy evaluation and use it to estimate 𝑣𝜋\n",
    "for\n",
    "the grid world in Exercise gw/simple, where 𝜋 is the equiprobable\n",
    "random policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b01588b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cca66263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=0):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90fc9016",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid:\n",
    "    \n",
    "    def __init__(self, w, h):\n",
    "        self._w = w\n",
    "        self._h = h\n",
    "        \n",
    "        self.R = -np.ones((w, h))\n",
    "        self.actions = {\n",
    "            \"up\": 0,\n",
    "            \"down\": 1,\n",
    "            \"left\": 2,\n",
    "            \"right\": 3\n",
    "        }\n",
    "        \n",
    "        self.terminal = [(0,0), (w-1, h-1)]\n",
    "                    \n",
    "    def sprime(self, s:np.array, a:int) -> tuple:\n",
    "        # actions: (0,1,2,3) = (up, down, left, right)\n",
    "        actions = {\n",
    "            0: np.array((-1, 0)),\n",
    "            1: np.array((1, 0)),\n",
    "            2: np.array((0, -1)),\n",
    "            3: np.array((0, 1))\n",
    "        }\n",
    "        s_prime = s + actions[a]\n",
    "        s_prime[0] = max(min(s_prime[0], self._w-1), 0)\n",
    "        s_prime[1] = max(min(s_prime[1], self._h-1), 0)\n",
    "        \n",
    "        return tuple(s_prime)\n",
    "\n",
    "    def reward(self, s:np.array, a: int) -> float:\n",
    "        s_prime = self.sprime(s, a)\n",
    "        reward = self.R[s_prime[0], s_prime[1]]\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def cumulative_reward(self, s, π, γ=0.9, max_steps=1):\n",
    "        if s in self.terminal:\n",
    "            return 0\n",
    "        reward = 0\n",
    "        \n",
    "        for a in range(π.shape[0]):\n",
    "            sprime = self.sprime(s, a)\n",
    "            \n",
    "            if max_steps > 1:\n",
    "                reward += π[a, s[0], s[1]]*(self.reward(s, a) + γ*self.cumulative_reward(sprime, π, γ, max_steps-1))\n",
    "            else:\n",
    "                reward += π[a, s[0], s[1]]*self.reward(s, a)\n",
    "        return reward\n",
    "\n",
    "    def iterative_policy_evaluation(self, π, γ=1.0, θ=0.1, max_steps=1000):\n",
    "        V = np.random.rand(self._w, self._h)      \n",
    "        for t in self.terminal:\n",
    "            V[t] = 0\n",
    "\n",
    "        Δ = θ+1\n",
    "        steps = 0\n",
    "        while Δ > θ:\n",
    "            Δ = 0\n",
    "            for x in range(self._w):\n",
    "                for y in range(self._h):\n",
    "                    s = (x, y)\n",
    "                    if s in self.terminal:\n",
    "                        continue\n",
    "                    \n",
    "                    v = V[s]\n",
    "                    v_new = 0\n",
    "                    \n",
    "                    # The additional sum over s', r can be ignored in our case because our enviroment is deterministic\n",
    "                    # i.e. for a given state s and action a the outcome is s'(a) and r(s')\n",
    "                    for a in range(π.shape[0]):\n",
    "                        sprime = self.sprime(s, a)\n",
    "                        v_new += π[a, x, y]*(self.reward(s, a) + γ*V[sprime])\n",
    "                        \n",
    "                    V[s] = v_new\n",
    "                    Δ = max(Δ, abs(v - V[s]))\n",
    "                    \n",
    "            steps += 1\n",
    "            if steps > max_steps:\n",
    "                print(f'Reached max step. {Δ=}.')\n",
    "                return V\n",
    "            \n",
    "        print(f'{steps=}')\n",
    "        return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7543a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, h = 4, 4\n",
    "\n",
    "g = Grid(w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fae8f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A policy function is a matrix \"policy\" with shape (#actions, grid_w, grid_h) \n",
    "# where policy[a, x, y] is the probability of adction a in state (x, y).\n",
    "# actions: (1,2,3,4) = (up, down, left, right)\n",
    "\n",
    "# π = softmax(np.random.rand(4, w, h)); # random policy\n",
    "π = softmax(np.ones((4, w, h))); # equiprob policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cefc4ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps=2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0., -1., -1., -1.],\n",
       "       [-1., -1., -1., -1.],\n",
       "       [-1., -1., -1., -1.],\n",
       "       [-1., -1., -1.,  0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.iterative_policy_evaluation(π, γ=0, θ=0.001)\n",
    "# plt.imshow(g.iterative_policy_evaluation(π, γ=0, θ=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fcf08e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps=88\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.        , -13.99314976, -19.99015187, -21.9891603 ],\n",
       "       [-13.99314976, -17.99159386, -19.99087286, -19.99097723],\n",
       "       [-19.99015187, -19.99087286, -17.99229837, -13.99424987],\n",
       "       [-21.98916031, -19.99097723, -13.99424987,   0.        ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.iterative_policy_evaluation(π, γ=1, θ=0.001, max_steps=200)\n",
    "# plt.imshow(g.iterative_policy_evaluation(π, γ=0.99, θ=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62857eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d98a6c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimal policy\n",
    "π = softmax(np.ones((4, w, h))); # equiprob policy\n",
    "for x in range(w):\n",
    "    for y in range(h):\n",
    "        dy = (h-1) - y\n",
    "        dx = (w-1) - x\n",
    "        \n",
    "        if min(x+y,dx+dy) == x+y:\n",
    "            if y==0:\n",
    "                π[:,x,y] = (1,0,0,0)\n",
    "            else:\n",
    "                π[:,x,y] = (0,0,1,0)\n",
    "                \n",
    "        elif min(x+y,dx+dy) == dx+dy:\n",
    "            if dy==0:\n",
    "                π[:,x,y] = (0,1,0,0)\n",
    "            else:\n",
    "                π[:,x,y] = (0,0,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c66f10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps=2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0., -1., -1., -1.],\n",
       "       [-1., -1., -1., -1.],\n",
       "       [-1., -1., -1., -1.],\n",
       "       [-1., -1., -1.,  0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.iterative_policy_evaluation(π, γ=0, θ=0.001)\n",
    "# plt.imshow(g.iterative_policy_evaluation(π, γ=0, θ=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ccabd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps=3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0., -1., -2., -3.],\n",
       "       [-1., -2., -3., -2.],\n",
       "       [-2., -3., -2., -1.],\n",
       "       [-3., -2., -1.,  0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.iterative_policy_evaluation(π, γ=1, θ=0.001)\n",
    "# plt.imshow(g.iterative_policy_evaluation(π, γ=1, θ=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e780328a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
